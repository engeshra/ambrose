package com.twitter.ambrose.cascading;

import DotGraphParser.CascadingEdge;
import DotGraphParser.DotParser;
import cascading.CascadingNotifier;
import cascading.flow.BaseFlow;
import cascading.flow.Flow;
import cascading.flow.FlowStep;
import cascading.flow.Flows;
import cascading.flow.hadoop.planner.HadoopFlowStepJob;
import cascading.flow.planner.BaseFlowStep;
import cascading.flow.planner.FlowStepGraph;
import cascading.flow.planner.FlowStepJob;
import cascading.flow.planner.Scope;
import cascading.management.state.ClientState;
import cascading.stats.hadoop.HadoopStepStats;
import com.twitter.ambrose.model.JobInfo;
import com.twitter.ambrose.model.WorkflowInfo;
import com.twitter.ambrose.service.DAGNode;
import com.twitter.ambrose.service.StatsWriteService;
import com.twitter.ambrose.service.WorkflowEvent;
import com.twitter.ambrose.util.JSONUtil;
import java.io.FileNotFoundException;
import java.util.logging.Level;
import java.util.logging.Logger;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;

//import org.apache.commons.logging.Log;
//import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.RunningJob;
import org.apache.hadoop.mapred.TaskReport;

import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import java.util.TreeMap;
import org.apache.commons.logging.Log;
import org.jgrapht.GraphHelper;
import org.jgrapht.Graphs;
import org.jgrapht.graph.SimpleDirectedGraph;

public class AmbroseCascadingProgressNotificationListener implements CascadingNotifier {

    private static final String RUNTIME = "Cascading";
    private StatsWriteService statsWriteService;
    private String workflowVersion;
    private List<JobInfo> jobInfoList = new ArrayList<JobInfo>();
    private Map<String, DAGNode> dagNodeNameMap = new TreeMap<String, DAGNode>();
    private HashSet<String> completedJobIds = new HashSet<String>();
    protected Log log = LogFactory.getLog(getClass());

    protected static enum WorkflowProgressField {

        workflowProgress;
    }

    protected static enum JobProgressField {

        jobId, jobName, trackingUrl, isComplete, isSuccessful,
        mapProgress, reduceProgress, totalMappers, totalReducers;
    }

    /**
     * Intialize this class with an instance of StatsWriteService to push stats to.
     *
     * @param statsWriteService
     */
    public AmbroseCascadingProgressNotificationListener(StatsWriteService statsWriteService) {
        this.statsWriteService = statsWriteService;
    }

    protected StatsWriteService getStatsWriteService() {
        return statsWriteService;
    }

    @Override
    public void onStarting(Flow flow) {
        BaseFlow bf = (BaseFlow) flow;
        List<BaseFlowStep> steps = bf.getFlowSteps();

        // to write a dot file of jobs graph
        //bf.writeDOT("dagFile");
        //bf.writeStepsDOT("step");

        //Flows is a utility helper class to call the protected method getFlowStepGraph.
        Flows flows = null;
        // AmbroseCascadingGraphConvertor to convert the graph generated by cascading to
        // DAGNodes Graph to be sent to ambrose
        AmbroseCascadingGraphConvertor convertor = new AmbroseCascadingGraphConvertor((SimpleDirectedGraph) flows.getStepGraphFrom(flow), dagNodeNameMap);
        convertor.convert();


        try {
            statsWriteService.sendDagNodeNameMap(null, this.dagNodeNameMap);
        } catch (IOException e) {
            log.error("Couldn't send dag to StatsWriteService", e);
        }

    }

    @Override
    public void onStopping(Flow flow) {
        // TODO Auto-generated method stub

    }

    @Override
    public void onCompleted(Flow flow) {
        // TODO Auto-generated method stub

    }

    @Override
    public boolean onThrowable(Flow flow, Throwable throwable) {
        // TODO Auto-generated method stub
        return false;
    }

    @Override
    public void onJobCompleted(String jobId) {
        //get job node
        Set<String> keys = this.dagNodeNameMap.keySet();
        for (String key : keys) {
            DAGNode node = ((DAGNode) dagNodeNameMap.get(key));
            if (node != null && node.getJobId() != null && node.getJobId().equals(jobId)) {
                pushEvent(jobId, WorkflowEvent.EVENT_TYPE.JOB_FINISHED, node);
                return;
            }
        }

    }

    @Override
    public void onJobFailed(String jobId) {
        Set<String> keys = this.dagNodeNameMap.keySet();
        for (String key : keys) {
            DAGNode node = ((DAGNode) dagNodeNameMap.get(key));
            if (node != null && node.getJobId().equals(jobId)) {
                pushEvent(jobId, WorkflowEvent.EVENT_TYPE.JOB_FAILED, node);
                return;
            }
        }

    }

    @Override
    public void onJobProgress(String jobName, String jobId, FlowStepJob<?> flowStepJob) {
        HadoopFlowStepJob h = (HadoopFlowStepJob) flowStepJob;
        HadoopStepStats h2 = ((HadoopStepStats) h.createStepStats(ClientState.NULL));
        JobClient jc = h2.getJobClient();
        RunningJob rj = h2.getRunningJob();

        Map<JobProgressField, String> progressMap = buildJobStatusMap(rj, jc);
        if (progressMap != null) {
            pushEvent(jobId, WorkflowEvent.EVENT_TYPE.JOB_PROGRESS, progressMap);
        }

        if ("true".equals(progressMap.get(JobProgressField.isComplete))) {
            completedJobIds.add(jobId);
        }


    }

    public void onJobStarted(String jobName, String jobId, FlowStepJob<?> flowStepJob) {
        //get hadoop running job
        HadoopFlowStepJob h = (HadoopFlowStepJob) flowStepJob;
        HadoopStepStats h2 = ((HadoopStepStats) h.createStepStats(ClientState.NULL));
        RunningJob rj = h2.getRunningJob();
        JobClient jc = h2.getJobClient();

        DAGNode node = this.dagNodeNameMap.get(jobName);
        if (node == null) {
            log.warn("jobStartedNotification - unrecorgnized operator name found  for jobId " + jobId);
            System.out.println("name = " + jobName);
            System.out.println(dagNodeNameMap);
        } else {
            node.setJobId(jobId);

            pushEvent(jobId, WorkflowEvent.EVENT_TYPE.JOB_STARTED, node);

            Map<JobProgressField, String> progressMap = buildJobStatusMap(rj, jc);
            if (progressMap != null) {
                pushEvent(jobId, WorkflowEvent.EVENT_TYPE.JOB_PROGRESS, progressMap);
            }

        }
    }

    private void pushEvent(String scriptId, WorkflowEvent.EVENT_TYPE eventType, Object eventData) {
        try {
            statsWriteService.pushEvent(scriptId, new WorkflowEvent(eventType, eventData, RUNTIME));
        } catch (IOException e) {
            log.error("Couldn't send event to StatsWriteService", e);
        }
    }

    @SuppressWarnings("deprecation")
    private Map<JobProgressField, String> buildJobStatusMap(RunningJob rj, JobClient jc) {
        try {
            JobClient jobClient = jc;
            JobID jobID = rj.getID();
            TaskReport[] mapTaskReport = jobClient.getMapTaskReports(jobID);
            TaskReport[] reduceTaskReport = jobClient.getReduceTaskReports(jobID);
            Map<JobProgressField, String> progressMap = new HashMap<JobProgressField, String>();
            progressMap.put(JobProgressField.jobId, jobID.toString());
            progressMap.put(JobProgressField.jobName, rj.getJobName());
            progressMap.put(JobProgressField.trackingUrl, rj.getTrackingURL());
            progressMap.put(JobProgressField.isComplete, Boolean.toString(rj.isComplete()));
            progressMap.put(JobProgressField.isSuccessful, Boolean.toString(rj.isSuccessful()));
            progressMap.put(JobProgressField.mapProgress, Float.toString(rj.mapProgress()));
            progressMap.put(JobProgressField.reduceProgress, Float.toString(rj.reduceProgress()));
            progressMap.put(JobProgressField.totalMappers, Integer.toString(mapTaskReport.length));
            progressMap.put(JobProgressField.totalReducers, Integer.toString(reduceTaskReport.length));
            return progressMap;
        } catch (IOException ex) {
            Logger.getLogger(AmbroseCascadingProgressNotificationListener.class.getName()).log(Level.SEVERE, null, ex);
        }
        return null;
    }
}
